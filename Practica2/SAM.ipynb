{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a35e40a-98ee-43cc-b084-b58c92f6b511",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SAM\n",
    "\n",
    "Segment Anything és un projecte de **Meta** amb dues grans aportacions:\n",
    "\n",
    "- Un gran conjunt de dades per a la segmentació d'imatges\n",
    "- El model Segment Anything (SAM) com a model de base per a la segmentació d'imatges\n",
    "\n",
    "Va ser introduït a l'article **Segment Anything** per Alexander Kirillov _et al._** (Abril 2023) [enllaç](https://arxiv.org/pdf/2304.02643.pdf)\n",
    "\n",
    "Aquest model s'inspira en el camp de la NLP (_Natural Language Processing_), on la creació de models base (_foundation models_) i els grans conjunts de dades (per valor de milers de milions de dades) s'han convertit en la manera habitual de fer feina.\n",
    "\n",
    "\n",
    "Com ja sabem, la segmentació d'imatges té diversos usos, aquests inclouen: l'anàlisi d'imatges biomèdiques, l'edició de fotografies i la conducció autònoma, entre d'altres. Per resoldre qualsevol d'aquests problemes, cal entrenar models especialitzats per cada una de les tasques que hem citat (és més, per cada un dels subproblemes que es deriven de cada tasca i per cada cas en particular). Això requereix un ampli coneixement del domini del problema i el temps necessari per a la recollida de dades específiques, per no parlar de les hores d'entrenament i ajustamen que són necessàries per als models d'aprenentatge profund.\n",
    "\n",
    "Enllaços:\n",
    "- [Repositòri Oficial](https://github.com/facebookresearch/segment-anything)\n",
    "- [Demo for dummies](https://segment-anything.com/demo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d141c08f-5c68-49de-a68e-a2c791dd34a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model\n",
    "\n",
    "SAM és un model d'aprenentatge profund (basat en _transformers_). I com amb qualsevol aprenentatge profund, s'ha entrenat en un gran nombre d'imatges i màscares: **més de mil milions de màscares en 11 milions d'imatges**.\n",
    "\n",
    "Com SAM és una model base, i està preparat per segmentar qualsevol tipus d'imatge, pot rebre indicacions dels usuaris sobre quina àrea segmentar. Actualment podem proporcionar tres indicacions diferents a SAM:\n",
    "\n",
    "- Indicar punts que contenen i no contenen el que volem segmentar.\n",
    "- Dibuixant un quadre delimitador (_bounding box_).\n",
    "- Dibuixant una màscara genearl sobre un objecte.\n",
    "\n",
    "En l'article és parla de preparar un _promt_ per poder especificar el que es vol segmentar emprant text.\n",
    "\n",
    "L'arquitectura de la xarxa és la següent:\n",
    "\n",
    "![SAM](https://learnopencv.com/wp-content/uploads/2023/04/segment-anything-model.png)\n",
    "\n",
    "\n",
    "Les caracterìstiques més importants del model (secció 3 de l'article) són:\n",
    "\n",
    "**Encoder**. Vision Transformer (ViT) basat en la idea d'un MAE (Masked AutoEncoder) El codificador d'imatge s'executa una vegada per imatge i es pot aplicar abans de sol·licitar el\n",
    "_prompt_ al model.\n",
    "\n",
    "**Codificador de _prompts_**. S'en consideren dos tipus: dispersos (punts, _bounding boxes_, text) i densos (màscares). Representen els punts i les _bounding boxes_ mitjançant codificacions posicionals amb altres informacions apreses per a cada tipus d'_input_ i text de forma lliure amb un codificador de text anomenat CLIP. Les indicacions denses (és a dir, les màscares) s'incorporen mitjançant l'ús convolucions i es sumen (concatenen) amb la codificació obtinguda de l'encoder de la imatge.\n",
    "\n",
    "**Descodificador de màscares**. El descodificador de màscara mapeja de manera eficient els _embeddings_ d'una imatge i el resultat del codificador de _prompts_ utilitzant una modificació d'un bloc descodificador Transformer \n",
    "\n",
    "**Resolució de l'ambigüitat**. Amb una sortida, el model farà una mitjana de diverses màscares vàlides si se li dóna una indicació ambigua. Per solucionar-ho, modifiquem el model per predir múltiples màscares de sortida per a un sol _prompt_ d'entreada. Experimentalment s'ha arribat a la conclusió que 3 màscares de sortida de màscara són suficients per abordar els casos més habituals (les màscares imbricades solen tenir tres profunditats com a màxim: senceres, parcials i subparts). \n",
    "\n",
    "**Eficiència**. El disseny global del model està motivat en gran mesura per l'eficiència. Donat un _embedding_ precalculat d'una imatge, el codificador de _promts_ i el descodificador de màscares s'executen en un navegador web, a la CPU, en ∼50 ms. Aquest rendiment en temps d'execució permet una indicació interactiva en temps real del model.\n",
    "\n",
    "**Pèrdues i entrenament.** Es supervisa la predicció de les màscares amb la combinació lineal d'una funció de pèrdua focal [enllaç](https://paperswithcode.com/method/focal-loss) i una funció de pèrdua de dice (ja emprada al model YOLO). L'entrenament es realitza utilitzant una barreja d'indicacions geomètriques, es simula un entrenament interactiu mitjançant un mostreig aleatori de prompts en 11 rondes per màscara.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff5ea1-0000-4226-8d72-55c015bf5235",
   "metadata": {},
   "source": [
    "## Dades\n",
    "\n",
    "Com ja sabem, la base de qualsevol model d'aprenentatge profund innovador és el conjunt de dades en què s'ha entrenat. El conjunt de dades de SAM conté més d'**11 milions d'imatges i 1.100 milions de màscares**. El conjunt de dades final s'anomena conjunt de dades SA-1B.\n",
    "\n",
    "Segurament es necessita aquest conjunt de dades per entrenar un model de capacitat de Segment Anything. Però també sabem que aquests conjunts de dades no existeixen i que és impossible anotar manualment tantes imatges.Per tant es necessitar l'ajut de SAM per anotar el conjunt de dades: Els anotadors de dades van utilitzar SAM per anotar imatges de manera interactiva i les dades anotades es van utilitzar per entrenar SAM. Aquest procés es va repetir, cosa que va donar lloc al motor de dades en bucle de SAM.\n",
    "\n",
    "Aquest motor de dades + formació del SAM al conjunt de dades té tres etapes:\n",
    "\n",
    "- Etapa Manual Assistida\n",
    "- Etapa semiautomàtica\n",
    "- Etapa totalment automàtica\n",
    "\n",
    "En la primera etapa, els anotadors van utilitzar un model SAM prèviament entrenat per segmentar objectes de manera interactiva en imatges al navegador. Els _embeddings_ de les imatges es van calcular prèviament per fer que el procés d'anotació fos fluid i en temps real. Després de la primera etapa, el conjunt de dades constava de 4,3 milions de màscares a partir  de 120.000 imatges. El model Segment Anything es va tornar a entrenar en aquest conjunt de dades.\n",
    "\n",
    "En la segona etapa semiautomàtica, els objectes destacats ja estaven segmentats mitjançant SAM. Els anotadors també van anotar objectes menys destacats que no tenien anotació. Aquesta etapa va donar lloc a 5,9 milions de màscares addicionals en 180.000 imatges en les quals es va tornar a entrenar SAM.\n",
    "\n",
    "A l'etapa \"totalment automàtica\" final, l'anotació la va fer íntegrament SAM. En aquesta etapa, ja s'havia entrenat el model en més de 10 milions de màscares que ho van fer possible. La generació automàtica de màscares es va aplicar 11M a imatges , donant lloc a 1,1B  màscares.\n",
    "En paraules del seus creadors:\n",
    "\n",
    "\"La versió final del conjunt de dades Segment Anything el converteix en el conjunt de dades de segmentació d'imatges més gran disponible públicament. En comparació amb OpenImages V5, hi ha 6 vegades més imatges i 400 vegades més màscares al conjunt de dades.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f297ba-8cae-493d-a16b-ee8638826be6",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "\n",
    "Hi ha 3 models diferents de SAM:\n",
    "\n",
    "- ViT-B SAM\n",
    "- ViT-L SAM\n",
    "- ViT-H SAM\n",
    "\n",
    "Com ja vàrem veure l'altra dia amb YOLO, el projecte també té paquet que podem instal·lar l mitjançant l'ordre següent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9785518-4fd0-4e6a-af03-158ce4b9bb08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088cc29-bd5c-428f-a8a7-37953601e863",
   "metadata": {},
   "source": [
    "Nosaltres utilitzarem els pesos i el model SAM oficials per executar inferències en algunes imatges. Podeu trobar tots els pesos oficials del model al repositori oficial [enllaç](https://github.com/facebookresearch/segment-anything#model-checkpoints).\n",
    "\n",
    "Anem a veure un exemple dús del model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5361752-9d11-4a91-b27e-5671a02d4a82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50847020-ba39-4ed0-8888-2fee0976694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1278cef-219d-44f5-a6f9-0e774fc96197",
   "metadata": {},
   "source": [
    "A continuació teniu algunes funcions de suport per dibuixar i ajudar a comprendre el resultat. Aquestes funcions són de la documentació oficial [aquí](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89514626-8259-483e-b4b9-01a8ef21267f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        img = np.ones((m.shape[0], m.shape[1], 3))\n",
    "        color_mask = np.random.random((1, 3)).tolist()[0]\n",
    "        for i in range(3):\n",
    "            img[:,:,i] = color_mask[i]\n",
    "        np.dstack((img, m*0.35))\n",
    "        ax.imshow(np.dstack((img, m*0.35)))\n",
    "        \n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d7f2d-6933-4146-b278-135e0ca38ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "torch.manual_seed(33)\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932b8554-e5fd-4efa-8fa3-aa4754190cd4",
   "metadata": {},
   "source": [
    "Anem a predir totes les màscares d'una imatge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cbfa89-7ee1-4430-903c-07e292212663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = \"tony_gaga.jpg\"\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36a8144-0e27-41a2-bf96-2080637e72ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sam = sam_model_registry[\"vit_b\"](checkpoint=\"sam_vit_b_01ec64.pth\") # Si teniu capacitat de càlcul provau amb el model vit_h\n",
    "sam.to(device)\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)  # genera totes les màscares\n",
    " \n",
    "\n",
    "masks = mask_generator.generate(image)\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "show_anns(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e581929a-c8dd-4335-a7e4-4931c2ba43ed",
   "metadata": {},
   "source": [
    "Ara emprarem el _prompt_ per indicar un objecte a segmentar. L'estructura bàsica és la següent:\n",
    "\n",
    "```\n",
    "sam = sam_model_registry[\"<model_type>\"](checkpoint=\"<path/to/checkpoint>\")\n",
    "predictor = SamPredictor(sam)\n",
    "predictor.set_image(<your_image>)\n",
    "masks, _, _ = predictor.predict(<input_prompts>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16118e06-99dc-491a-9942-687cae3d39b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry # necessitam importar el predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5366b2-f903-4490-91be-6f7046990c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sam = sam_model_registry[\"vit_b\"](checkpoint=\"sam_vit_b_01ec64.pth\")\n",
    "sam.to(device)\n",
    "predictor = SamPredictor(sam) # genera les màscares que li demanam\n",
    "predictor.set_image(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe1698-1ff5-43c4-86a8-46915ca1823d",
   "metadata": {},
   "source": [
    "Per seleccionar un objecte hem de triar un punt. Els punts s'introdueixen al model en format (x,y) i vénen amb les etiquetes 1 (punt d'un objecte) o 0 (punt de fons). Es poden introduir diversos punts; aquí només en fem servir un. El punt escollit es mostrarà com una estrella a la imatge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aedffa-6e9b-4416-9e94-3268c2a4e650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_point = np.array([[500, 500]])\n",
    "input_label = np.array([1])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('on')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00ddb14-07db-438e-af6b-d8db7ae01fc1",
   "metadata": {},
   "source": [
    "Farem la predicciió amb `SamPredictor.predict`. El model retorna màscares, prediccions de qualitat per a aquestes màscares i _logits_  (prediccions) de màscares de baixa resolució que es poden passar a la següent iteració de predicció."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7deb100-2ab0-44d9-8aba-71364ccd354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    multimask_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c95cc-082a-40fd-ae8b-5faf5513c8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c8a1d-3410-4ba2-ba0f-a9ae8028e938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    show_mask(mask, plt.gca())\n",
    "    show_points(input_point, input_label, plt.gca())\n",
    "    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef5983c-61b6-4b0e-8bc9-cc4a958bf6b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feina a fer\n",
    "\n",
    "Seguir el tutorial [aquí](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb) per poder fer prediccions emprant _bounding boxes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d5cae-8166-449f-a6ac-5e171fb42414",
   "metadata": {},
   "source": [
    "\n",
    "**Extra 1**\n",
    "\n",
    "També podeu trobar el model i  altres fitxers de pesos en el repositori de huggingface ([enllaç](https://github.com/facebookresearch/segment-anything#model-checkpoints)) que a més ens aporta un conjunt de funcions i llibreries adicionals per tasques d'aprenentatge profund.\n",
    "\n",
    "**Extra 2**\n",
    "\n",
    "[Tutorial Roboflow](https://blog.roboflow.com/how-to-use-segment-anything-model-sam/)\n",
    "\n",
    "_In this written tutorial (and the video below), we will explore how to use SAM to generate masks automatically, create segmentation masks using bounding boxes, and convert object detection datasets into segmentation masks. If you're interested in using SAM to label data for computer vision, Roboflow Annotate uses SAM to power automated polygon labeling in the browser which you can try for free._\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
