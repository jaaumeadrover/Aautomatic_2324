{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam, SGD\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pylab as pl\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentació d'objectes emprant Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Una manera diferent d'activar la GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feina a fer**\n",
    "\n",
    "Un cop teniu els conjunts de dades creats heu de comprovar que les imatges que es corresponen amb les etiquetes tenen la informació correcta, feis una visualització."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Dades entrenament\n",
    "path_train = # PUT YOUR PATH\n",
    "files = os.listdir(path_train)\n",
    "img_files = list([f\"{path_train}{p}\" for p in files if p.endswith('.png')])[:500]\n",
    "label_files = list([f\"{path_train}gt/{p}\" for p in files if p.endswith('.png')])[:500]\n",
    "print(\"total training images\", len(img_files))\n",
    "\n",
    "# Dades validacio\n",
    "\n",
    "path_val = # PUT YOUR PATH\n",
    "files = os.listdir(path_val)\n",
    "img_files_val = list([f\"{path_val}{p}\" for p in files if p.endswith('.png')])\n",
    "label_files_val = list([f\"{path_val}gt/{p}\" for p in files if p.endswith('.png')])\n",
    "print(\"total test images\", len(img_files_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 100\n",
    "\n",
    "# Definim una seqüència (composició) de transformacions \n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ## TODO: Put if necessary\n",
    "    ])\n",
    "\n",
    "# Constructor del dataset.\n",
    "class Formes(Dataset):\n",
    "    def __init__(self, images, labels, transform):\n",
    "        super().__init__()\n",
    "        self.paths = images\n",
    "        self.labels = labels\n",
    "        self.len = len(self.paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.len\n",
    "    \n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        path = self.paths[index]\n",
    "        label = self.labels[index]\n",
    "        image = cv2.imread(path) #, cv2.IMREAD_GRAYSCALE)  # Depén de vosaltres\n",
    "        \n",
    "        image = self.transform(image)\n",
    "        label_img = cv2.imread(label)\n",
    "\n",
    "        merged_image = cv2.add(label_img[:,:,0], cv2.add(label_img[:,:,1], label_img[:,:,2]))\n",
    "        \n",
    "        label_img = self.transform(merged_image)\n",
    "        return image,  #### TODO\n",
    "    \n",
    "train_data = Formes(img_files, label_files, transform)\n",
    "val_data = Formes(img_files_val, label_files_val, transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, train_batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iterador =  iter(val_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features, labels = next(iterador)\n",
    "\n",
    "print(\"Saber l'estructura del batch us ajudarà: \")\n",
    "print(f\"Feature batch shape: {features.size()}\")\n",
    "print(f\"Labels batch shape: {labels.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: mostrar una imatge del batch i devora mostrar l'imatge que fa d'etiqueta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definició de la xarxa\n",
    "\n",
    "Podem observar com es pot emprar l'orientació a objectes de **Python** per crear una xarxa de manera ordenada, és interessant analitzar aquest codi amb detall ja que en podem aprendre molt:\n",
    "\n",
    "Aquí tenim 2 capes noves que ens ajudaràn a construïr la nova arquitectura:\n",
    "\n",
    "- [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html): Aplica un operador de convolució transposat 2D sobre una imatge d'entrada composta per diversos plans d'entrada.    [Exemples gràfics](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md).\n",
    "- [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html): És una tècnica utilitzada en l'entrenament de xarxes neuronals artificials per a estabilizar i accelerar el procés de convergència durant l'entrenament. Bàsicament, durant l'entrenament d'una red neuronal, els valors d'entrada de cada capa poden canviar a mesura que els paràmetres de les capes anteriors s'actualitzen. Això pot fer que l'entrenament sigui més lento o inestable. La normalització per lots resuelve això normalitzant les activacions de cada capa abans de passar a la següent capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "        \n",
    "        ## CODER\n",
    "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "        \n",
    "        ## DECODER\n",
    "        \n",
    "        \n",
    "        # TODO: Construeix el teu decoder\n",
    "        \n",
    "        self.upconv4 = nn.ConvTranspose2d(¿?, ¿?, kernel_size=2, stride=2)  # Empra aquesta capa com exemple\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "\n",
    "        \n",
    "        return torch.sigmoid()  # Aplicarem una sigmoide a la sortida de la xarxa -> TODO: Recordar el que ha dit en biel a classe\n",
    "\n",
    "    \n",
    "    # Ara ja podem començar a fer coses amb cara i ulls\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (name + \"conv1\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=in_channels,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    (name + \"conv2\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=features,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenament\n",
    "\n",
    "Per fer tasques de segmentació, una de les funcions de pèrdua que podem emprar és el _Diceloss_ (intersecció vs unió):  El coeficient de _Dice_ s'utilitza habitualment en tasques de segmentació d'imatges com a mesura de la superposició entre les màscares de segmentació entre la predicció i el _ground truth_. El  _Diceloss_, és el complementari del coeficient de _Dice_, es pot utilitzar com a funció de pèrdua per entrenar models per a tasques de segmentació.\n",
    "\n",
    "Dice Coefficient=$ = 2 \\times \\frac{|X \\cap Y|}{|X| + |Y|}$\n",
    "\n",
    "\n",
    "\n",
    "On:\n",
    "\n",
    "- $X$ és la màscara de segmentació prevista.\n",
    "- $Y$ és la màscara de segmentació de la veritat del sòl.\n",
    "- $∣⋅∣$ denota la cardinalitat o el nombre d'elements d'un conjunt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = 0.0\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.size() == y_true.size()\n",
    "        y_pred = y_pred[:, 0].contiguous().view(-1)\n",
    "        y_true = y_true[:, 0].contiguous().view(-1)\n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        dsc = (2. * intersection + self.smooth) / (\n",
    "            y_pred.sum() + y_true.sum() + self.smooth\n",
    "        )\n",
    "        return 1. - dsc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El bucle d'entrenament és diferent al que estau acostumats a veure en l'assignatura, s'assembla molt més als propis tutorials de _Pytorch_.\n",
    "\n",
    "A més s'aprofita per introduir la visualització de resultats de forma dinàmica usant la llibreria [tqdm](https://github.com/tqdm/tqdm) i la llibreria _matplotlib_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "torch.manual_seed(33)\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "\n",
    "model = UNet().to(device)\n",
    "\n",
    "optim = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = DiceLoss() \n",
    "\n",
    "t_loss = np.zeros((epochs))\n",
    "v_loss = np.zeros((epochs))\n",
    "\n",
    "pbar = tqdm(range(1, epochs+1)) # tdqm permet tenir text dinàmic\n",
    "\n",
    "for epoch in pbar:\n",
    "    \n",
    "    train_loss = 0 \n",
    "    val_loss = 0  \n",
    "    \n",
    "    model.train()                                                  \n",
    "    for batch_num, (input_img, target) in enumerate(train_loader, 1):   \n",
    "        input_img= input_img.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(input_img)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()                                            \n",
    "        optim.step()                                               \n",
    "        optim.zero_grad()     \n",
    "        \n",
    "        train_loss += loss.item()    \n",
    "                                                       \n",
    "    model.eval()   \n",
    "    with torch.no_grad():                                          \n",
    "        for input_img, target in val_loader: \n",
    "            input_img = input_img.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = model(input_img)                                   \n",
    "            loss = criterion(output, target)   \n",
    "            val_loss += loss.item()  \n",
    "    \n",
    "    # RESULTATS\n",
    "    train_loss /= len(train_loader)\n",
    "    t_loss[epoch-1] = train_loss\n",
    "    \n",
    "    val_loss /= len(val_loader)   \n",
    "    v_loss[epoch-1] = val_loss\n",
    "    \n",
    "    # VISUALITZACIO DINAMICA\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    pl.plot(t_loss[:epoch], label=\"train\")\n",
    "    pl.plot(v_loss[:epoch], label=\"validation\")\n",
    "    pl.legend()\n",
    "    pl.xlim(0, epochs)\n",
    "    pl.xticks(range(0,epochs,1),range(1,epochs+1,1))\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())\n",
    "    plt.close()\n",
    "\n",
    "    pbar.set_description(f\"Epoch:{epoch} Training Loss:{train_loss} Validation Loss:{val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardam el model, d'aquesta manera no es necessari fer l'entrenament a classe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"unet.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaluació"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregam el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel =  UNet().to(device)\n",
    "mmodel.load_state_dict(torch.load(\"unet.pt\"))\n",
    "mmodel.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feina a fer**\n",
    "\n",
    "Visualitzar exemples de segmentació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "[agraïments Unet](https://github.com/mateuszbuda/brain-segmentation-pytorch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
