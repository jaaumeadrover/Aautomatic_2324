{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1ozwAAiAyNHA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMy35hII78XT"
   },
   "source": [
    "# Transferència de coneixement\n",
    "\n",
    "L'objectiu d'avui és aprendre com podem emprar arquitectures ja existents per resoldre els nostres problemes. \n",
    "\n",
    "Com objectius secundaris tenim:\n",
    "\n",
    "1. Conèixer un nou conjunt de dades\n",
    "2. Entendre en profunditat com són dos dels models més emprats.\n",
    "3. Guardar i carregar xarxes neuronals\n",
    "\n",
    "## Dades\n",
    "\n",
    "El conjunt de dades [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) consta de 60.000 imatges en color de 32x32 pixels etiquetades en 10 classes, amb 6.000 imatges per classe. Hi ha 50.000 imatges d'entrenament i 10.000 imatges de _test_.\n",
    "\n",
    "\n",
    "Si voleu normalitzar les dades, a continuació teniu els valors ja calculats:\n",
    "\n",
    "  - mitjana: (0.4914, 0.4822, 0.4465)\n",
    "  - desviació típica: (0.247, 0.243, 0.261)\n",
    "\n",
    "Una altra funció que pot ser útil és `Resize(mida_desti)` que rep un enter com a paràmetre (la mida final).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjdicviT7-dg",
    "outputId": "2b758f54-06ba-4d2c-ae2d-f90bd36a2570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 100\n",
    "\n",
    "# Definim una seqüència (composició) de transformacions \n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(227)\n",
    "    ])\n",
    "\n",
    "# Descarregam un dataset ja integrat en la llibreria Pytorch\n",
    "train_data = datasets.CIFAR10('../data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10('../data', train=False, transform=transform)\n",
    "\n",
    "# Transformam les dades en l'estructura necessaria per entrenar una xarxa\n",
    "train_loader = torch.utils.data.DataLoader(train_data, train_batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dty0xrAh71Qw"
   },
   "source": [
    "## Transfer learning (Definició de la xarxa)\n",
    "\n",
    "En aquesta pràctica aplicarem la tècnica de _transfer learning_ a partir de dues de les xarxes més conegudes en el camp de visió per computador:\n",
    "\n",
    "-[**AlexNet**](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf). (ImageNet Classification with Deep Convolutional Neural Network, 2012). La mida d'entrada de les imatges és de (227x227x3).Té prop de 60 milions de paràmetres entrenables.\n",
    "\n",
    "[**MobileNet v3**](https://pytorch.org/vision/main/models/mobilenetv3.html): és una arquitectura de xarxa neuronal convolucional dissenyada per a aplicacions de visió per ordinador en dispositius mòbils i sistemes encastats. Va ser desenvolupada per Google a la seva sèrie MobileNet, que se centra en l'eficiència i el rendiment de les xarxes neuronals en dispositius amb recursos limitats, com ara telèfons mòbils. Té dues versions: _small_ amb configuracions que van dels 2 als 5 milions de paràmetres; _large_ amb uns 6 milions de paràmetres. La mida d'entrada de les imatges és de (224x224x3)\n",
    "\n",
    "_Pytorch_ ens permet emprar aquestes xarxes de manera molt senzilla. [Més informació](https://pytorch.org/vision/stable/models.html).\n",
    "\n",
    "Si el model que cercam no es troba integrat dins la llibreria _Pytorch_ és bastant probable que si la trobem a Huggingface.\n",
    "\n",
    "Descarregarem AlexNet i a analitzar-la. En aquest cas no només ens baixam la seva arquitectura, també els pesos resultants de l'entrenament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "xaoFxi7cygHX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaume\\miniconda3\\envs\\au2023\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Arquitectura AlexNet\n",
      "--------------------------------------------------\n",
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "alex = models.alexnet(weights=True)\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Arquitectura AlexNet\")\n",
    "print(\"-\"*50)\n",
    "print(alex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asepjghw2xED"
   },
   "source": [
    "Hi ha diverses maneres de realitzar la tècnica de _TransferLearning_ les dues més conegudes són:\n",
    "\n",
    " - **\"Congelar\"** els pesos de la part d'extracció de característiques (la part convolucional) i crear un nou classificador que s'adapti al nostre problema. Això implica que només entrenam una part de la xarxa.\n",
    " - **Reentrenar tota la xarxa**.\n",
    "\n",
    "Com que la nostra capacitat de càlcul és limitada, ens decantarem per la primera opció. Pensau que\n",
    "\n",
    " Per tal d'evitar el reentrenament necessitam canviar el valor de l'atribut  `requires_grad` al valor `False`. Aquest atribut és propietat de cada tensor. Podem recorrer els tensors mitjançant el següent codi:\n",
    " ```\n",
    "for param in alex.features.parameters():\n",
    "    param.requires_grad = False\n",
    " ```\n",
    " ### Feina a fer:\n",
    "\n",
    "- Bloc 1\n",
    " 1. Carregar la xarxa AlexNet i seleccionar la part d'extracció de característiques.\n",
    " 2. Definir un entorn seqüencial on implementarem el classificador de la xarxa.\n",
    " 3. Realitzar un entrenament: comparar rendiment (accuracy) i nombre de paràmetres.\n",
    " 4. Provar de guardar la vostra xarxa i tornar-la a carregar. Classificar una imatge del conjunt de test.\n",
    "- Bloc 2: Repetir el mateix procés que l'anterior amb la xarxa mobilenet v3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZrzR1-4Ny3hx"
   },
   "outputs": [],
   "source": [
    "#TODO Congelar\n",
    "for param in alex.features.parameters():\n",
    "   param.requires_grad = False\n",
    "    \n",
    "my_net =  nn.Sequential(#TODO posar ,\n",
    "                        alex.features,\n",
    "                        alex.avgpool,\n",
    "                        nn.Flatten(1,-1),\n",
    "                        nn.Linear(9216,4096),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(4096,2048),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(2048,512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(512,10),\n",
    "                        nn.Dropout(p=0.5),\n",
    "                        nn.ReLU()\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZH80zEa8IPW"
   },
   "source": [
    "## Entrenament\n",
    "\n",
    "[shhht](https://github.com/tqdm/tqdm) si voleu canviar el resum de l'entrenament per una barra de progrés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "eJiXfzTM7e8d"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=100, verbose=True):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    loss_v = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target, reduction='mean') \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0 and verbose:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Average: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), loss.item()/ len(data)))\n",
    "        loss_v += loss.item()\n",
    "\n",
    "    loss_v /= len(train_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}\\n'.format(loss_v))\n",
    " \n",
    "    return loss_v\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='mean') \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    " \n",
    "  \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "llV5gCGU7jIT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters  47197706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaume\\miniconda3\\envs\\au2023\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.300824, Average: 0.035950\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 2.302585, Average: 0.035978\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 2.302585, Average: 0.035978\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 2.302585, Average: 0.035978\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 2.302585, Average: 0.035978\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 2.302585, Average: 0.035978\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 2.302585, Average: 0.035978\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 2.302585, Average: 0.035978\n",
      "\n",
      "Train set: Average loss: 0.0418\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0230, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.302585, Average: 0.035978\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.302585, Average: 0.035978\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 2.302585, Average: 0.035978\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 2.302585, Average: 0.035978\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 2.302585, Average: 0.035978\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 2.302585, Average: 0.035978\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 2.302585, Average: 0.035978\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 2.302585, Average: 0.035978\n",
      "\n",
      "Train set: Average loss: 0.0360\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0230, Accuracy: 1000/10000 (10%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_cuda = False\n",
    "torch.manual_seed(33)\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "epochs = 2\n",
    "lr = 0.01\n",
    "\n",
    "model = my_net.to(device)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad) # !!!\n",
    "\n",
    "print(\"Parameters \", pytorch_total_params)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Guardam el valor de pèrdua mig de cada iteració (època)\n",
    "train_l = np.zeros((epochs))\n",
    "test_l = np.zeros((epochs))\n",
    "\n",
    "# Bucle d'entrenament\n",
    "for epoch in range(0, epochs):\n",
    "    train_l[epoch] = train(model, device, train_loader, optimizer, epoch)\n",
    "    test_l[epoch]  = test(model, device, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJzao3Z7Jlc_"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Resultats de l'entrenament\")\n",
    "plt.plot(range(1, (epochs + 1)), train_l,  c=\"red\", label=\"train\")\n",
    "plt.plot(range(1,  (epochs + 1)), test_l,  c=\"green\", label=\"test\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
